{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e602f8-3b53-477b-88b2-35c76e5585e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, length, when\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "277c26e9-9314-4dfa-9283-263e70aec929",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SentimentAnalysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dddbd1a5-41f1-421d-b001-9c0c5d2a3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(\"Tweets.csv\", header=True, encoding=\"ISO-8859-1\", inferSchema=True)\n",
    "df2 = spark.read.csv(\"train.csv\", header=True, encoding=\"ISO-8859-1\", inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f5e5974-7b30-45b8-b4a4-cbfaca3f46f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tweet_id: string, airline_sentiment: string, airline_sentiment_confidence: string, negativereason: string, negativereason_confidence: string, airline: string, airline_sentiment_gold: string, name: string, negativereason_gold: string, retweet_count: int, text: string, tweet_coord: string, tweet_created: string, tweet_location: string, user_timezone: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f6466da-7c4a-42ce-924a-571dd8232a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[textID: string, text: string, selected_text: string, sentiment: string, Time of Tweet: string, Age of User: string, Country: string, Population -2020: int, Land Area (Km²): double, Density (P/Km²): int]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fbdb0dd-2aea-4e7e-80ca-f08d5f7c9c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Step 1: Clean and label df1 (airline dataset)\n",
    "df1 = df1.select(\n",
    "    col(\"text\"),\n",
    "    col(\"airline_sentiment\").alias(\"sentiment\")\n",
    ").withColumn(\"source\", when(col(\"text\").isNotNull(), \"airline\"))\n",
    "\n",
    "# Step 2: Clean and label df2 (general dataset)\n",
    "df2 = df2.select(\n",
    "    col(\"text\"),\n",
    "    col(\"sentiment\")\n",
    ").withColumn(\"source\", when(col(\"text\").isNotNull(), \"general\"))\n",
    "\n",
    "# Step 3: Combine and drop nulls\n",
    "combined_df = df1.unionByName(df2).na.drop(subset=[\"text\", \"sentiment\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d864398-8dda-4a53-9c2a-5641e0b4cb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[text: string, sentiment: string, source: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "962a27e4-a114-4791-96b1-810af9248886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+-----------+----------+----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                              |tokens                                                                                                                                              |filtered                                                                                                            |text_length|word_count|is_airline|features                                                                                                                                                                                                                                                                                                                                         |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+-----------+----------+----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|@VirginAmerica What @dhepburn said.                                                                                               |[@virginamerica, what, @dhepburn, said.]                                                                                                            |[@virginamerica, @dhepburn, said.]                                                                                  |35         |4         |1         |(5003,[2341,3018,4098,5000,5001,5002],[19.500186367549187,8.961381253535766,48.37757014119364,0.8818865635182896,0.5496786696257737,2.1000792257670295])                                                                                                                                                                                         |\n",
      "|@VirginAmerica plus you've added commercials to the experience... tacky.                                                          |[@virginamerica, plus, you've, added, commercials, to, the, experience..., tacky.]                                                                  |[@virginamerica, plus, added, commercials, experience..., tacky.]                                                   |72         |9         |1         |(5003,[1517,1860,3018,3307,3946,4290,5000,5001,5002],[24.373882993128873,17.56041581745317,8.961381253535766,35.206544623053894,25.87273274969656,37.47672563991625,1.8141666449519103,1.2367770066579908,2.1000792257670295])                                                                                                                   |\n",
      "|@VirginAmerica I didn't today... Must mean I need to take another trip!                                                           |[@virginamerica, i, didn't, today..., must, mean, i, need, to, take, another, trip!]                                                                |[@virginamerica, today..., must, mean, need, take, another, trip!]                                                  |71         |12        |1         |(5003,[855,909,1023,1037,1248,1537,3018,3166,5000,5001,5002],[8.749975961883097,15.156860088059599,15.038776253995318,29.03758194138034,38.792956818826575,6.0770776257378465,8.961381253535766,8.149982090285594,1.7889698859942447,1.6490360088773213,2.1000792257670295])                                                                     |\n",
      "|\"@VirginAmerica it's really aggressive to blast obnoxious \"\"entertainment\"\" in your guests' faces &amp; they have little recourse\"|[\"@virginamerica, it's, really, aggressive, to, blast, obnoxious, \"\"entertainment\"\", in, your, guests', faces, &amp;, they, have, little, recourse\"]|[\"@virginamerica, really, aggressive, blast, obnoxious, \"\"entertainment\"\", guests', faces, &amp;, little, recourse\"]|130        |17        |1         |(5003,[208,345,536,993,1216,1590,2055,2377,4112,4706,4972,5000,5001,5002],[11.390547922551876,26.732871466670055,6.765990652435053,10.949551991059005,41.053143244755866,10.64221178870699,31.679586687767408,43.76123802522074,5.506720133513595,22.40995919293452,15.958465105244692,3.2755786644965044,2.3361343459095383,2.1000792257670295])|\n",
      "|@VirginAmerica and it's a really big bad thing about it                                                                           |[@virginamerica, and, it's, a, really, big, bad, thing, about, it]                                                                                  |[@virginamerica, really, big, bad, thing]                                                                           |55         |10        |1         |(5003,[2956,3018,3048,3857,4112,5000,5001,5002],[8.531707381481542,8.961381253535766,10.164722245427594,11.640553286811205,5.506720133513595,1.385821742671598,1.3741966740644342,2.1000792257670295])                                                                                                                                           |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+-----------+----------+----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    RegexTokenizer, StopWordsRemover, HashingTF, IDF,\n",
    "    VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.sql.functions import col, length, size, when\n",
    "\n",
    "\n",
    "# Convert to string just in case\n",
    "combined_df = combined_df.withColumn(\"processed_text\", col(\"text\").cast(\"string\"))\n",
    "\n",
    "# Create text_length feature\n",
    "combined_df = combined_df.withColumn(\"text_length\", length(col(\"text\")))\n",
    "\n",
    "# Create is_airline binary feature\n",
    "combined_df = combined_df.withColumn(\"is_airline\", when(col(\"source\") == \"airline\", 1).otherwise(0))\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = RegexTokenizer(inputCol=\"processed_text\", outputCol=\"tokens\", pattern=\"\\\\s+\")\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "\n",
    "# TF-IDF\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"tf\", numFeatures=5000)\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "\n",
    "# Add word count after tokens\n",
    "# We'll use a SQL transformer after tokenizer if needed — but here's a cleaner solution:\n",
    "# Use the `size()` function on the tokens column *after* tokenization\n",
    "\n",
    "# Build preprocessing pipeline up to tokenization for intermediate step\n",
    "token_pipeline = Pipeline(stages=[tokenizer])\n",
    "token_model = token_pipeline.fit(combined_df)\n",
    "tokenized_df = token_model.transform(combined_df)\n",
    "\n",
    "# Now add `word_count`\n",
    "tokenized_df = tokenized_df.withColumn(\"word_count\", size(col(\"tokens\")))\n",
    "\n",
    "# Re-apply the rest of the pipeline on tokenized_df\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"tf\", numFeatures=5000)\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"tfidf\", \"text_length\", \"word_count\", \"is_airline\"],\n",
    "    outputCol=\"assembled_features\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"assembled_features\", outputCol=\"features\")\n",
    "\n",
    "# Final pipeline (after tokenization)\n",
    "final_pipeline = Pipeline(stages=[remover, hashingTF, idf, assembler, scaler])\n",
    "final_model = final_pipeline.fit(tokenized_df)\n",
    "preprocessed_df = final_model.transform(tokenized_df)\n",
    "\n",
    "\n",
    "# Display key columns\n",
    "preprocessed_df.select(\n",
    "    \"text\", \"tokens\", \"filtered\", \"text_length\", \"word_count\", \"is_airline\", \"features\"\n",
    ").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b78f4596-7eaa-43d5-afb1-c047711d5ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[text: string, sentiment: string, source: string, processed_text: string, text_length: int, word_count: int, is_airline: int, tokens: array<string>, filtered: array<string>, tf: vector, tfidf: vector, assembled_features: vector, features: vector]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8254116c-a060-4872-84d6-542e93e4ef58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8700\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Prepare scaler and classifier\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "lr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"label\", maxIter=20)\n",
    "\n",
    "# Pipeline\n",
    "pipeline_lr = Pipeline(stages=[scaler, lr])\n",
    "\n",
    "# Train model\n",
    "model_lr = pipeline_lr.fit(preprocessed_df)\n",
    "\n",
    "# Predict\n",
    "predictions_lr = model_lr.transform(preprocessed_df)\n",
    "\n",
    "# Evaluate\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy_lr = evaluator.evaluate(predictions_lr)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb05ee12-8960-4149-a533-e2cf82e80c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8460\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"scaled_features\", labelCol=\"label\", numTrees=50)\n",
    "\n",
    "pipeline_rf = Pipeline(stages=[scaler, rf])\n",
    "\n",
    "model_rf = pipeline_rf.fit(preprocessed_df)\n",
    "\n",
    "predictions_rf = model_rf.transform(preprocessed_df)\n",
    "\n",
    "accuracy_rf = evaluator.evaluate(predictions_rf)\n",
    "\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ff020b8-4bfc-47a8-a614-2b95a2d17251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.8587\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol=\"scaled_features\", labelCol=\"label\")\n",
    "\n",
    "pipeline_dt = Pipeline(stages=[scaler, dt])\n",
    "\n",
    "model_dt = pipeline_dt.fit(preprocessed_df)\n",
    "\n",
    "predictions_dt = model_dt.transform(preprocessed_df)\n",
    "\n",
    "accuracy_dt = evaluator.evaluate(predictions_dt)\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {accuracy_dt:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6617250c-e1c4-4322-8fa2-cc84ea0f331a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.8347\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Assuming 'features' column exists\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "nb = NaiveBayes(featuresCol=\"scaled_features\", labelCol=\"label\")\n",
    "\n",
    "pipeline_nb = Pipeline(stages=[scaler, nb])\n",
    "\n",
    "model_nb = pipeline_nb.fit(preprocessed_df)\n",
    "\n",
    "predictions_nb = model_nb.transform(preprocessed_df)\n",
    "\n",
    "accuracy_nb = evaluator.evaluate(predictions_nb)\n",
    "\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_nb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4ef767f-9515-4fee-809d-cdedb7820239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM One-vs-Rest Accuracy: 0.8605\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import OneVsRest, LinearSVC\n",
    "\n",
    "# Initialize the base classifier\n",
    "svm = LinearSVC(featuresCol=\"scaled_features\", labelCol=\"label\")\n",
    "\n",
    "# One-vs-Rest classifier\n",
    "ovr = OneVsRest(classifier=svm, labelCol=\"label\", featuresCol=\"scaled_features\")\n",
    "\n",
    "pipeline_svm = Pipeline(stages=[scaler, ovr])\n",
    "\n",
    "model_svm = pipeline_svm.fit(preprocessed_df)\n",
    "\n",
    "predictions_svm = model_svm.transform(preprocessed_df)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions_svm)\n",
    "print(f\"SVM One-vs-Rest Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e486b5-82d8-4662-9ccb-6a0d87345fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
